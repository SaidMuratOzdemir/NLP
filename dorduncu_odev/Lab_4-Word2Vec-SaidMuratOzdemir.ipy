# Bu script'te verilen teaching/learning korpusu uzerinde Word2Vec egitimi
# yapiyorum. Anahtar kelimeler icin en benzer kelimeleri bulacagim,
# PCA ile gorsellestirecegim ve stop-words etkisini inceleyecegim.

import re
import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.decomposition import PCA

# Stop-words: NLTK varsa indir, yoksa manuel liste kullan
try:
    import nltk
    from nltk.corpus import stopwords
    nltk.download('stopwords', quiet=True)
    STOPWORDS = set(stopwords.words('english'))
    print("Stop-words başarıyla yüklendi!")
except Exception:
    print("NLTK indirilemedi, manuel stop-words kullanıyorum")
    STOPWORDS = {
        'the','and','is','are','am','be','been','being','a','an','to','of','in','on','for','with','at','by','from','as',
        'that','this','it','its','into','through','during','within','between','over','under','above','below','than','then',
        'so','too','very','can','will','just','not','only','but','also','now','such','own','same','no','nor','do','does',
        'did','doing','have','has','had','having','because','while','if','until','again','further','once','who','whom',
        'which','what','when','where','why','how','all','any','both','each','few','more','most','other','some','your',
        'yours','yourself','yourselves','our','ours','ourselves','their','theirs','themselves','i','me','my','mine','we',
        'us','he','him','his','she','her','hers','they','them','you'
    }

# Sonuclarin ayni cikmasi icin seed belirliyorum
SEED = 42

# Adim 1: Korpus dosyasini oku
from pathlib import Path

candidates = [
    Path('dorduncu_odev/teaching_learning_corpus.txt'),
    Path('teaching_learning_corpus.txt'),
    Path.cwd() / 'teaching_learning_corpus.txt',
    Path.cwd().parent / 'dorduncu_odev' / 'teaching_learning_corpus.txt',
]
path = next((p for p in candidates if p.exists()), None)
if path is None:
    raise FileNotFoundError('teaching_learning_corpus.txt bulunamadi. Calisma dizini: ' + str(Path.cwd()))
CORPUS_PATH = str(path)

with open(CORPUS_PATH, 'r', encoding='utf-8') as f:
    raw_text = f.read()
print('Kullanılan korpus yolu:', CORPUS_PATH)
print('Toplam karakter sayısı:', len(raw_text))
print('\nİlk 200 karakter:')
print(raw_text[:200])

# Adim 2: Tokenizasyon fonksiyonu (kucuk harf + noktalama temizleme)

def tokenize(s: str):
    s = s.lower()  # hepsini kucuk harf yap
    s = re.sub(r'[^a-z\s]', ' ', s)  # sadece harf ve bosluk birak
    toks = [t for t in s.split() if t]  # bosluklara gore ayir
    return toks

# Metni cümlelere ayır (nokta ve satır sonlarından)
raw_sents = re.split(r'[\n\.]+' , raw_text)
sentences = [tokenize(s) for s in raw_sents if s.strip()]
print('Toplam cümle sayısı:', len(sentences))
print('Toplam kelime sayısı:', sum(len(s) for s in sentences))
print('\nİlk 2 cümle:')
print(sentences[:2])

# Adim 3: Word2Vec modelini egit
# Parametreler:
# - vector_size=50: her kelime 50 boyutlu vektor
# - window=5: 5 kelime context penceresi
# - min_count=1: en az 1 kez gecen kelimeleri al (korpus kucuk)
# - sg=1: skip-gram kullan
# - epochs=300: 300 kez egit

w2v_base = Word2Vec(
    sentences=sentences, 
    vector_size=50, 
    window=5,
    min_count=1, 
    workers=1, 
    sg=1, 
    epochs=300, 
    seed=SEED
)
vocab_size = len(w2v_base.wv)
print('Modeldeki toplam kelime sayısı:', vocab_size)

# Adim 4: Anahtar kelimeler icin en benzer 10 kelimeyi bul
keywords = ['university', 'teacher', 'student', 'learning']

def print_topk(model, word, k=10):
    if word in model.wv:
        sims = model.wv.most_similar(word, topn=k)
        for t, sc in sims:
            print(f"  {t:15s} benzerlik: {sc:.4f}")
        return sims
    else:
        print('  Kelime korpusta yok (out-of-vocabulary)')
        return []

print("=== EN BENZER 10 KELİME ===\n")
baseline_neighbors = {}
for w in keywords:
    print(w.upper() + ':')
    baseline_neighbors[w] = print_topk(w2v_base, w, k=10)
    print()

# Adim 5: "student" kelimesinin vektor temsilinin ilk 10 sayisi
if 'student' in w2v_base.wv:
    vec = w2v_base.wv['student'][:10]
    print('STUDENT kelimesinin vektörünün ilk 10 elemanı:')
    print('[', ', '.join(f'{x:.4f}' for x in vec), ']')
else:
    print('student kelimesi korpusta bulunamadı')

# Adim 6: Bir test kelimesi sec ve en yakin 3 komsuyu bul
# "learning" seçtim çünkü korpusun ana konusu bu
test_word = 'learning'
print(f'\nTEST: "{test_word}" kelimesinin en yakın 3 komşusu:\n')
if test_word in w2v_base.wv:
    sims3 = w2v_base.wv.most_similar(test_word, topn=3)
    for i, (t, sc) in enumerate(sims3, 1):
        print(f'{i}. {t:15s} (benzerlik: {sc:.4f})')
else:
    print('Kelime bulunamadı!')

# Adim 7: PCA ile 2 boyutlu gorsellestirme
def plot_pca(model, seed_words, topn=5, title='Word2Vec - PCA Görselleştirme'):
    # Anahtar kelimeleri ve yakin komsularini sec
    selected = set()
    for w in seed_words:
        if w in model.wv:
            selected.add(w)
            for t, _ in model.wv.most_similar(w, topn=topn):
                selected.add(t)
    
    if not selected:
        print('Hic kelime bulunamadi!')
        return
    
    # Vektor matrisini olustur
    labels = list(selected)
    X = np.array([model.wv[w] for w in labels])
    
    # PCA ile 50 boyuttan 2 boyuta indir
    pca = PCA(n_components=2, random_state=SEED)
    XY = pca.fit_transform(X)
    
    # Grafigi ciz
    plt.figure(figsize=(10,8))
    plt.scatter(XY[:,0], XY[:,1], s=50, alpha=0.6, c='blue')
    for (x,y), lab in zip(XY, labels):
        plt.text(x+0.02, y+0.02, lab, fontsize=10)
    plt.title(title)
    plt.xlabel('PC1 (1. bileşen)')
    plt.ylabel('PC2 (2. bileşen)')
    plt.grid(True, alpha=0.3)
    plt.show()

print('\nPCA görselleştirmesi oluşturuluyor...')
plot_pca(w2v_base, keywords, topn=5, title='PCA Görselleştirme - İlk Model')

# Adim 8: Stop-words'leri cikarip modeli yeniden egit
print('\n=== STOP-WORDS ÇIKARTILARAK YENİDEN EĞİTİM ===\n')

sentences_sw = [[t for t in s if t not in STOPWORDS] for s in sentences]

w2v_sw = Word2Vec(
    sentences=sentences_sw, 
    vector_size=50, 
    window=5,
    min_count=1, 
    workers=1, 
    sg=1, 
    epochs=300, 
    seed=SEED
)

print('Stop-words çıkarıldıktan sonra sözlük boyutu:', len(w2v_sw.wv))
print('Önceki sözlük boyutu:', vocab_size)
print(f'Fark: {vocab_size - len(w2v_sw.wv)} kelime azaldı')

print("\n=== STOP-WORDS ÇIKARILMIŞ MODEL - EN BENZER 10 KELİME ===\n")
for w in keywords:
    print(w.upper() + ':')
    print_topk(w2v_sw, w, k=10)
    print()

print('Stop-words çıkarılmış modelin PCA görselleştirmesi...')
plot_pca(w2v_sw, keywords, topn=5, title='PCA Görselleştirme - Stop-words Çıkarılmış Model')
