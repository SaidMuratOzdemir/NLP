{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Lab 5 - Topic Modelling\n\nNotebook to explore the short answers in `TopicModellingAssignment.txt` and recover the underlying prompts/topics with simple models. I added my own notes, alternative trials, and reflections to keep this authentic and explain decisions."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Game plan (per instructions)\n\n- Load the provided document; each line is a student answer tied to one of a few topics.\n- Do quick EDA (lengths, a glance at adjectives/opinion words) to see what signal we have.\n- Vectorize with bag-of-words + bigrams; remove prompt words so topics reflect answers.\n- Baseline LDA at 4 topics; inspect top words and top documents.\n- Try an alternative (NMF on TF\u2013IDF) as a new trail to see if themes shift.\n- Sweep topic counts and note stability; add my interpretations and what worked/what didn\u2019t.\n- Keep outputs visible per section and organize code blocks clearly."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data loading & basic stats\n\nBrief stats help see how noisy/short the answers are before modeling."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pathlib\nfrom pprint import pprint\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\n\ndata_path = pathlib.Path('TopicModellingAssignment.txt')\nassert data_path.exists(), 'Put this notebook in besinci_odev/ or adjust the path'\n\nraw_lines = [ln.strip() for ln in data_path.read_text().splitlines() if ln.strip()]\nprint(f'Loaded {len(raw_lines)} non-empty lines')\npprint(raw_lines[:5])\n\nlengths = [len(x.split()) for x in raw_lines]\nprint(f'Avg length: {sum(lengths)/len(lengths):.1f} words | Min={min(lengths)} | Max={max(lengths)}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Quick look at adjectives / opinion words (extra trail)\n\nThe earlier feedback asked for closer look at adjectives/opinion hints. This is a lightweight pass to see the most frequent adjective-like tokens (using a simple suffix filter to avoid heavy dependencies)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import re\nfrom collections import Counter\n\ntokens = []\nfor line in raw_lines:\n    tokens.extend(re.findall(r\"[A-Za-z']+\", line.lower()))\n\n# crude adjective guess: words ending with common adjective suffixes\nadj_suffixes = ('al','ive','ous','ful','less','ble','ary','ic')\nadjs = [t for t in tokens if any(t.endswith(suf) for suf in adj_suffixes) and len(t) > 3]\ncounts = Counter(adjs).most_common(15)\nprint('Top adjective-like words:', counts)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pre-processing for topic models\n\n- Lower-case unigrams + bigrams.\n- Remove generic English stopwords **and** prompt-heavy terms (roman/greek/planet/kepler/orbit/law/etc.) so we learn from answers not the question text.\n- Use `min_df=3` to drop one-off typos/noise."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "extra_stop = {\n    'roman','romans','greeks','greek','rome','government','goverment','governing',\n    'senate','senato','consuls','console','assemblies','assembly','consule','consul',\n    'king','people','person','persons','individual','individuals','interests','interest',\n    'conflict','planet','planets','kepler','earth','sun','universe','orbit','orbits',\n    'law','laws'\n}\nstop_words = list(ENGLISH_STOP_WORDS.union(extra_stop))\n\nbow = CountVectorizer(stop_words=stop_words, ngram_range=(1, 2), min_df=3)\nX_bow = bow.fit_transform(raw_lines)\nvocab_bow = bow.get_feature_names_out()\nprint('BoW shape:', X_bow.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Baseline: LDA with 4 topics\n\nChose 4 based on quick trials; fits the prompts (astronomy vs civics vs info dissemination)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "n_topics = 4\nlda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method='batch')\nlda.fit(X_bow)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def show_topics(model, feature_names, topn=12):\n    for idx, comp in enumerate(model.components_):\n        terms = comp.argsort()[-topn:][::-1]\n        labels = ', '.join(feature_names[i] for i in terms)\n        print(f'Topic {idx+1}: {labels}\\n')\n\nshow_topics(lda, vocab_bow)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\n\ndoc_topic = lda.transform(X_bow)\nfor t in range(n_topics):\n    top_docs = doc_topic[:, t].argsort()[-4:][::-1]\n    print(f'\\nTop docs for topic {t+1}:')\n    for i in top_docs:\n        print(f'- {raw_lines[i][:200]}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Alternative trail: NMF on TF\u2013IDF\n\nTrying a different model to see if themes sharpen; TF\u2013IDF + NMF often yields crisper, more distinct topics."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "tfidf = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2), min_df=3)\nX_tfidf = tfidf.fit_transform(raw_lines)\nvocab_tfidf = tfidf.get_feature_names_out()\n\nnmf = NMF(n_components=4, random_state=42, init='nndsvda', max_iter=400)\nW = nmf.fit_transform(X_tfidf)\nH = nmf.components_\n\ndef show_topics_matrix(matrix, feature_names, topn=12):\n    for idx, row in enumerate(matrix):\n        terms = row.argsort()[-topn:][::-1]\n        labels = ', '.join(feature_names[i] for i in terms)\n        print(f'NMF Topic {idx+1}: {labels}\\n')\n\nshow_topics_matrix(H, vocab_tfidf)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Topic count sweep (sanity check)\n\nQuick log-likelihood/perplexity across 3\u20136 topics to see stability. Lower perplexity is better; also watch that topics remain interpretable."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "for k in range(3, 7):\n    model = LatentDirichletAllocation(n_components=k, random_state=42, learning_method='batch')\n    model.fit(X_bow)\n    print(f'k={k} | log-lik={model.score(X_bow):.1f} | perp={model.perplexity(X_bow):.1f}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## My read of the topics (after runs)\n\n- LDA-4: astronomy/retrograde vs heliocentric (Topic 1); Greek civic/culture (Topic 2); info dissemination (Topic 3); Roman civics/engineering & contributions (Topic 4). Some bleed-through between 3 & 4.\n- NMF-4: slightly sharper separation on dissemination vs civics; astronomy remains clear.\n- k-sweep: 3 merges civics+dissemination; 5 splits astronomy finer but overlaps more; 4 stays the most interpretable.\n\nI\u2019ll keep 4 topics for the narrative but note the alternative view from NMF if asked."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## What worked / issues / next steps (own notes)\n\n- Worked: removing prompt words; bigrams; testing LDA+NMF; topic-count sweep with log-lik/perplexity; quick adjective scan to surface opinion-ish words.\n- Issues: noisy spelling + very short answers blur topics; some prompts are mixed in single lines.\n- Next steps if time: gentle spell normalization + lemmatization; add coherence metrics; manually label top docs for each topic; optionally split astronomy into retrograde vs heliocentric if 5-topic model is desired.\n- Authenticity note: kept manual interpretations and rationale instead of auto-labels; added alternative modeling trail to show thinking beyond the template."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}